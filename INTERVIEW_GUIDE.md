# é¢è¯•é—®ç­”æŒ‡å—

æœ¬æ–‡æ¡£å¸®åŠ©ä½ å‡†å¤‡å…³äºè¿™ä¸ªç”µå½±æ¨èé¡¹ç›®çš„é¢è¯•é—®é¢˜ã€‚

## æ ¸å¿ƒé—®é¢˜å‡†å¤‡

### Q1: "What is a data lake in your case?"

**å®Œç¾å›ç­”**:

"åœ¨æˆ‘çš„ç”µå½±æ¨èé¡¹ç›®ä¸­ï¼Œæ•°æ®æ¹–æ˜¯ä¸€ä¸ª**åˆ†å±‚çš„S3å­˜å‚¨æ¶æ„**ï¼Œé‡‡ç”¨å…¸å‹çš„Bronze-Silver-Goldä¸‰å±‚æ¨¡å¼ï¼š

**Bronzeå±‚ï¼ˆåŸå§‹æ•°æ®ï¼‰**ï¼š
- å­˜å‚¨ä»Kafkaå®æ—¶æ‘„å–çš„åŸå§‹è¯„åˆ†äº‹ä»¶
- æ ¼å¼ï¼šJSONæ–‡ä»¶
- è·¯å¾„ï¼š`s3://movie-rec-data-lake/bronze/user-events/date=YYYY-MM-DD/`
- ä¿ç•™å®Œæ•´çš„åŸå§‹æ•°æ®ï¼Œæ”¯æŒæ•°æ®å›æº¯å’Œé‡æ–°å¤„ç†

**Silverå±‚ï¼ˆæ¸…æ´—æ•°æ®ï¼‰**ï¼š
- å­˜å‚¨å¤„ç†åçš„ç”¨æˆ·å’Œç”µå½±ç‰¹å¾
- æ ¼å¼ï¼šParquetåˆ—å¼å­˜å‚¨ï¼ˆä¼˜åŒ–æŸ¥è¯¢æ€§èƒ½ï¼‰
- è·¯å¾„ï¼š`s3://movie-rec-data-lake/silver/user-features/`
- ç”¨äºæ‰¹å¤„ç†å’Œæ¨¡å‹è®­ç»ƒ

**Goldå±‚ï¼ˆåˆ†ææ•°æ®ï¼‰**ï¼š
- å­˜å‚¨é¢„è®¡ç®—çš„ç›¸ä¼¼åº¦çŸ©é˜µå’Œèšåˆç»Ÿè®¡
- ç”¨äºé«˜çº§åˆ†æå’ŒæŠ¥è¡¨

è¿™ä¸ªæ•°æ®æ¹–é‡‡ç”¨**Schema-on-Read**æ¨¡å¼ï¼Œæ•°æ®å­˜å‚¨æ—¶ä¸å¼ºåˆ¶Schemaï¼Œè¯»å–æ—¶æ ¹æ®éœ€æ±‚çµæ´»è§£æã€‚"

---

### Q2: "Why did you send data to S3 instead of directly to a database?"

**å®Œç¾å›ç­”**:

"æˆ‘é‡‡ç”¨S3ä½œä¸ºæ•°æ®æ¹–æœ‰ä»¥ä¸‹å‡ ä¸ªå…³é”®åŸå› ï¼š

**1. æˆæœ¬æ•ˆç›Š**
- S3å­˜å‚¨æˆæœ¬æä½ï¼ˆ$0.023/GB/æœˆï¼‰ï¼Œé€‚åˆé•¿æœŸå­˜å‚¨å¤§é‡å†å²æ•°æ®
- å…³ç³»å‹æ•°æ®åº“å­˜å‚¨ç›¸åŒæ•°æ®æˆæœ¬é«˜10-50å€

**2. è§£è€¦å­˜å‚¨å’Œè®¡ç®—**
- S3ä½œä¸ºæŒä¹…åŒ–å±‚ï¼Œæ”¯æŒå¤šç§è®¡ç®—å¼•æ“è¯»å–ï¼ˆSparkã€Athenaã€EMRï¼‰
- æ•°æ®åº“æ•…éšœä¸ä¼šå¯¼è‡´æ•°æ®ä¸¢å¤±
- å¯ä»¥éšæ—¶é‡æ–°å¤„ç†å†å²æ•°æ®ï¼ˆreprocessingï¼‰

**3. Lambdaæ¶æ„**
- **Speed Layer**ï¼šKafka â†’ DynamoDBï¼ˆå®æ—¶ç‰¹å¾ï¼Œä½å»¶è¿Ÿï¼‰
- **Batch Layer**ï¼šKafka â†’ S3 â†’ Airflowï¼ˆæ‰¹å¤„ç†ï¼Œé«˜ååï¼‰
- S3ä½œä¸º"source of truth"ï¼ŒDynamoDBä½œä¸º"serving layer"

**4. æ•°æ®å®¡è®¡å’Œåˆè§„**
- S3æ”¯æŒç‰ˆæœ¬æ§åˆ¶å’Œä¸å¯å˜å­˜å‚¨
- æ»¡è¶³æ•°æ®æ²»ç†è¦æ±‚
- æ”¯æŒç¾éš¾æ¢å¤

**5. çµæ´»æ€§**
- åŸå§‹JSONä¿ç•™å®Œæ•´ä¿¡æ¯
- å¯ä»¥ç”¨ä¸åŒæ ¼å¼ä¼˜åŒ–ä¸åŒåœºæ™¯ï¼ˆParquetã€Avroï¼‰
- Schemaå˜æ›´æˆæœ¬ä½"

**ä»£ç ç¤ºä¾‹**:
```python
# S3ä¿å­˜åŸå§‹äº‹ä»¶ï¼ˆBronzeå±‚ï¼‰
def save_raw_event(self, event):
    key = f"bronze/user-events/date={date}/event_{uuid}.json"
    s3_client.put_object(
        Bucket='movie-rec-data-lake',
        Key=key,
        Body=json.dumps(event)
    )

# DynamoDBä¿å­˜å®æ—¶ç‰¹å¾ï¼ˆServingå±‚ï¼‰
def update_user_feature(self, user_id, features):
    dynamodb_table.put_item(
        Item={
            'user_id': user_id,
            'avg_rating': features['avg_rating'],
            'rating_count': features['rating_count']
        }
    )
```

---

### Q3: "How does your collaborative filtering work?"

**å®Œç¾å›ç­”**:

"æˆ‘å®ç°äº†**æ··åˆååŒè¿‡æ»¤**ï¼Œç»“åˆUser-basedå’ŒItem-basedä¸¤ç§æ–¹æ³•ï¼š

**User-based CFï¼ˆåŸºäºç”¨æˆ·ï¼‰**ï¼š
1. è®¡ç®—ç”¨æˆ·ç›¸ä¼¼åº¦ï¼ˆä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
2. æ‰¾åˆ°ä¸ç›®æ ‡ç”¨æˆ·æœ€ç›¸ä¼¼çš„Kä¸ªç”¨æˆ·ï¼ˆK=20ï¼‰
3. åŸºäºç›¸ä¼¼ç”¨æˆ·çš„è¯„åˆ†é¢„æµ‹ç›®æ ‡ç”¨æˆ·å¯¹ç”µå½±çš„è¯„åˆ†
4. å…¬å¼ï¼š`predicted_rating = Î£(similarity * rating) / Î£(similarity)`

**Item-based CFï¼ˆåŸºäºç‰©å“ï¼‰**ï¼š
1. è®¡ç®—ç”µå½±ç›¸ä¼¼åº¦çŸ©é˜µ
2. æ‰¾åˆ°ç”¨æˆ·å–œæ¬¢çš„ç”µå½±çš„ç›¸ä¼¼ç”µå½±
3. åŸºäºç›¸ä¼¼ç”µå½±çš„è¯„åˆ†é¢„æµ‹
4. å¯¹å†·å¯åŠ¨é—®é¢˜æ›´é²æ£’

**ä¼˜åŒ–ç­–ç•¥**ï¼š
- ç›¸ä¼¼åº¦ç¼“å­˜ï¼šé¿å…é‡å¤è®¡ç®—
- æœ€å°å…±åŒé¡¹é˜ˆå€¼ï¼šmin_common_items=3
- Top-Ké™åˆ¶ï¼šåªè€ƒè™‘æœ€ç›¸ä¼¼çš„20ä¸ªé‚»å±…

**ä»£ç å®ç°**ï¼ˆå…³é”®éƒ¨åˆ†ï¼‰ï¼š
```python
# è®¡ç®—ç”¨æˆ·ç›¸ä¼¼åº¦
def _cosine_similarity(vec1, vec2):
    dot_product = np.dot(vec1, vec2)
    norm1 = np.linalg.norm(vec1)
    norm2 = np.linalg.norm(vec2)
    return dot_product / (norm1 * norm2)

# ç”Ÿæˆæ¨è
def user_based_recommend(user_id, top_n=20):
    similar_users = get_similar_users(user_id)
    
    for movie_id in candidates:
        weighted_sum = 0
        similarity_sum = 0
        
        for similar_user, similarity in similar_users:
            if similar_user rated movie_id:
                weighted_sum += similarity * rating
                similarity_sum += similarity
        
        predicted_score = weighted_sum / similarity_sum
```

**æ€§èƒ½**ï¼š
- æ—¶é—´å¤æ‚åº¦ï¼šO(K * M)ï¼ŒKæ˜¯é‚»å±…æ•°ï¼ŒMæ˜¯å€™é€‰ç”µå½±æ•°
- ç©ºé—´å¤æ‚åº¦ï¼šO(U * K)ï¼ŒUæ˜¯ç”¨æˆ·æ•°ï¼Œç¼“å­˜Top-Ké‚»å±…"

---

### Q4: "How does your content-based model work?"

**å®Œç¾å›ç­”**:

"æˆ‘çš„åŸºäºå†…å®¹çš„æ¨èä½¿ç”¨**ç”µå½±ç‰¹å¾ç›¸ä¼¼åº¦**ï¼š

**æ ¸å¿ƒæ€æƒ³**ï¼š
- å¦‚æœç”¨æˆ·å–œæ¬¢æŸéƒ¨ç”µå½±ï¼Œæ¨èç›¸ä¼¼çš„ç”µå½±
- ç›¸ä¼¼åº¦åŸºäºå…¶ä»–ç”¨æˆ·çš„è¯„åˆ†æ¨¡å¼

**å®ç°æ­¥éª¤**ï¼š

1. **æ„å»ºç”µå½±ç›¸ä¼¼åº¦çŸ©é˜µ**ï¼š
   - ä½¿ç”¨ç”¨æˆ·-ç”µå½±è¯„åˆ†çŸ©é˜µ
   - è®¡ç®—ç”µå½±ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦
   - é¢„è®¡ç®—å¹¶ç¼“å­˜ï¼ˆæå‡æŸ¥è¯¢æ€§èƒ½ï¼‰

2. **åˆ†æç”¨æˆ·åå¥½**ï¼š
   - æå–ç”¨æˆ·çš„é«˜åˆ†ç”µå½±ï¼ˆrating â‰¥ 4.0ï¼‰
   - æ„å»ºç”¨æˆ·ç”»åƒ

3. **ç”Ÿæˆæ¨è**ï¼š
   - æ‰¾åˆ°ä¸ç”¨æˆ·å–œæ¬¢çš„ç”µå½±ç›¸ä¼¼çš„ç”µå½±
   - ä½¿ç”¨åŠ æƒå¹³å‡è®¡ç®—æ¨èåˆ†æ•°
   - å…¬å¼ï¼š`score = Î£(similarity * user_rating) / Î£(similarity)`

**ä»£ç å®ç°**ï¼š
```python
# é¢„è®¡ç®—ç”µå½±ç›¸ä¼¼åº¦çŸ©é˜µ
def _compute_movie_similarity_matrix(self):
    # è½¬ç½®çŸ©é˜µï¼šè¡Œä¸ºç”µå½±ï¼Œåˆ—ä¸ºç”¨æˆ·
    movie_matrix = rating_matrix.T
    
    # å½’ä¸€åŒ–
    normalized = movie_matrix / np.linalg.norm(movie_matrix, axis=1)
    
    # ä½™å¼¦ç›¸ä¼¼åº¦ = ç‚¹ç§¯
    similarity_matrix = np.dot(normalized, normalized.T)
    
    return similarity_matrix

# ç”Ÿæˆæ¨è
def recommend(user_id, top_n=20):
    liked_movies = get_user_high_rated_movies(user_id)
    
    for candidate_movie in all_movies:
        score = 0
        for liked_movie, rating in liked_movies:
            similarity = similarity_matrix[liked_movie, candidate_movie]
            score += similarity * rating
        
        normalized_score = score / len(liked_movies)
```

**ä¼˜åŠ¿**ï¼š
- è§£å†³ååŒè¿‡æ»¤çš„å†·å¯åŠ¨é—®é¢˜
- å¯ä»¥æ¨èæ–°ç”µå½±ï¼ˆåªè¦æœ‰ç›¸ä¼¼ç”µå½±ï¼‰
- æä¾›æ¨èè§£é‡Šèƒ½åŠ›"

---

### Q5: "How did you implement the hybrid model?"

**å®Œç¾å›ç­”**:

"æˆ‘å®ç°äº†**åŠ æƒæ··åˆæ¨èæ¨¡å‹**ï¼Œèåˆä¸‰ç§æ–¹æ³•ï¼š

**æ··åˆç­–ç•¥**ï¼š
- User-based CF: 30%æƒé‡
- Item-based CF: 30%æƒé‡  
- Content-based: 40%æƒé‡

**å®ç°é€»è¾‘**ï¼š
```python
def recommend(user_id, top_n=20):
    all_recommendations = {}
    
    # 1. User-based CFæ¨è
    user_cf_recs = self.cf_model.user_based_recommend(user_id)
    merge_with_weight(all_recommendations, user_cf_recs, weight=0.3)
    
    # 2. Item-based CFæ¨è
    item_cf_recs = self.cf_model.item_based_recommend(user_id)
    merge_with_weight(all_recommendations, item_cf_recs, weight=0.3)
    
    # 3. Content-basedæ¨è
    content_recs = self.content_model.recommend(user_id)
    merge_with_weight(all_recommendations, content_recs, weight=0.4)
    
    # 4. æŒ‰ç»¼åˆåˆ†æ•°æ’åº
    sorted_recs = sorted(all_recommendations.items(), 
                        key=lambda x: x[1], reverse=True)
    
    return [movie_id for movie_id, score in sorted_recs[:top_n]]
```

**é™çº§ç­–ç•¥**ï¼š
- æ–°ç”¨æˆ·æˆ–æ•°æ®ä¸è¶³æ—¶ï¼Œè¿”å›çƒ­é—¨ç”µå½±
- ä½¿ç”¨è´å¶æ–¯å¹³å‡è®¡ç®—çƒ­é—¨åº¦

**ä¸ºä»€ä¹ˆè¿™æ ·è®¾è®¡**ï¼š
- CFæ“…é•¿å‘ç°ç›¸ä¼¼ç”¨æˆ·/ç”µå½±çš„è§„å¾‹
- Content-basedè§£å†³å†·å¯åŠ¨é—®é¢˜
- æ··åˆå¯ä»¥å¹³è¡¡ä¸¤è€…ä¼˜ç¼ºç‚¹
- æƒé‡å¯é…ç½®ï¼Œæ”¯æŒA/Bæµ‹è¯•"

---

### Q6: "How do you handle the Kafka stream?"

**å®Œç¾å›ç­”**:

"æˆ‘ä½¿ç”¨**kafka-python**åº“å®ç°å®æ—¶æµå¤„ç†ï¼š

**æ¶ˆè´¹è€…æ¶æ„**ï¼š
```python
class MovieRecConsumer:
    def __init__(self, config):
        # Kafkaæ¶ˆè´¹è€…
        self.consumer = KafkaConsumer(
            'user-events',
            bootstrap_servers='localhost:9092',
            group_id='movie-rec-consumer-group',
            auto_offset_reset='earliest',
            value_deserializer=lambda x: json.loads(x)
        )
        
        # æ··åˆå­˜å‚¨å±‚
        self.storage = HybridStorage(config)
    
    def start(self):
        for message in self.consumer:
            event = message.value
            self.process_event(event)
    
    def process_event(self, event):
        # 1. ä¿å­˜åŸå§‹äº‹ä»¶åˆ°S3 Bronzeå±‚
        self.storage.save_rating(
            user_id=event['user_id'],
            movie_id=event['movie_id'],
            rating=event['rating'],
            timestamp=event['timestamp']
        )
        
        # 2. æ›´æ–°å®æ—¶ç‰¹å¾ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if self.mode == 'sqlite':
            self.feature_extractor.extract_features(event)
```

**äº‹ä»¶æ ¼å¼**ï¼š
```json
{
  "user_id": 123,
  "movie_id": 456,
  "rating": 4.5,
  "timestamp": "2025-03-15T10:30:00"
}
```

**å¯é æ€§ä¿è¯**ï¼š
- Consumer Groupï¼šæ”¯æŒå¹¶è¡Œæ¶ˆè´¹å’Œè´Ÿè½½å‡è¡¡
- Auto Commitï¼šè‡ªåŠ¨æäº¤offset
- Error Handlingï¼šå¼‚å¸¸æ•è·å’Œæ—¥å¿—è®°å½•
- At-least-onceè¯­ä¹‰ï¼šç¡®ä¿ä¸ä¸¢å¤±æ•°æ®"

---

### Q7: "How would you use Airflow for daily retraining?"

**å®Œç¾å›ç­”**:

"è™½ç„¶å½“å‰å®ç°æ²¡æœ‰Airflowï¼Œä½†æˆ‘è®¾è®¡äº†å®Œæ•´çš„æ‰¹å¤„ç†æ¶æ„ï¼š

**Airflow DAGè®¾è®¡**ï¼š
```python
from airflow import DAG
from airflow.operators.python_operator import PythonOperator

dag = DAG(
    'daily_feature_etl',
    schedule_interval='@daily',  # æ¯å¤©å‡Œæ™¨è¿è¡Œ
    start_date=datetime(2025, 3, 1)
)

# Task 1: æ•°æ®è´¨é‡æ£€æŸ¥
def validate_s3_data(**context):
    date = context['ds']  # æ˜¨å¤©çš„æ—¥æœŸ
    events = read_from_s3_bronze(date)
    
    # æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
    assert len(events) > 0
    assert all(validate_event(e) for e in events)

# Task 2: ç‰¹å¾å·¥ç¨‹
def extract_features(**context):
    events = read_from_s3_bronze(context['ds'])
    
    # è®¡ç®—ç”¨æˆ·ç‰¹å¾
    user_features = compute_user_features(events)
    
    # è®¡ç®—ç”µå½±ç‰¹å¾  
    movie_features = compute_movie_features(events)
    
    # ä¿å­˜åˆ°S3 Silverå±‚ï¼ˆParquetï¼‰
    save_to_s3_silver(user_features, movie_features)
    
    return {'user_count': len(user_features)}

# Task 3: æ›´æ–°DynamoDB
def update_feature_store(**context):
    features = read_from_s3_silver(context['ds'])
    
    # æ‰¹é‡æ›´æ–°DynamoDB
    batch_write_to_dynamodb(features)

# Task 4: æ¨¡å‹è®­ç»ƒ
def train_model(**context):
    # è¯»å–å†å²æ•°æ®
    data = read_training_data_from_s3()
    
    # è®­ç»ƒæ¨¡å‹
    model = HybridRecommender(data)
    
    # è¯„ä¼°æ¨¡å‹
    metrics = evaluate_model(model)
    
    # ä½¿ç”¨MLflowè·Ÿè¸ª
    with mlflow.start_run():
        mlflow.log_params(model.get_params())
        mlflow.log_metrics(metrics)
        mlflow.sklearn.log_model(model, "recommender")
    
    # å¦‚æœæ€§èƒ½æå‡ï¼Œéƒ¨ç½²åˆ°ç”Ÿäº§
    if metrics['hit_rate'] > current_best:
        deploy_model(model)

# Task 5: é€šçŸ¥
def send_notification(**context):
    metrics = context['task_instance'].xcom_pull(task_ids='train_model')
    send_email(f"Daily training completed: {metrics}")

# å®šä¹‰ä¾èµ–å…³ç³»
validate = PythonOperator(task_id='validate', python_callable=validate_s3_data, dag=dag)
extract = PythonOperator(task_id='extract', python_callable=extract_features, dag=dag)
update = PythonOperator(task_id='update', python_callable=update_feature_store, dag=dag)
train = PythonOperator(task_id='train', python_callable=train_model, dag=dag)
notify = PythonOperator(task_id='notify', python_callable=send_notification, dag=dag)

validate >> extract >> [update, train] >> notify
```

**ç›‘æ§å’Œå‘Šè­¦**ï¼š
- è®­ç»ƒå¤±è´¥æ—¶å‘é€å‘Šè­¦
- æ¨¡å‹æ€§èƒ½ä¸‹é™æ—¶é€šçŸ¥
- æ•°æ®è´¨é‡å¼‚å¸¸æ—¶æš‚åœpipeline"

---

### Q8: "How would you track experiments with MLflow?"

**å®Œç¾å›ç­”**:

"MLflowç”¨äº**æ¨¡å‹ç”Ÿå‘½å‘¨æœŸç®¡ç†**ï¼š

**å®éªŒè·Ÿè¸ª**ï¼š
```python
import mlflow

# å¼€å§‹å®éªŒ
with mlflow.start_run(run_name="hybrid_model_v1"):
    # 1. è®°å½•å‚æ•°
    mlflow.log_params({
        'n_neighbors': 20,
        'cf_weight': 0.6,
        'content_weight': 0.4,
        'min_common_items': 3
    })
    
    # 2. è®­ç»ƒæ¨¡å‹
    model = HybridRecommender(rating_matrix, config)
    
    # 3. è¯„ä¼°æ€§èƒ½
    metrics = evaluate_model(model, test_data)
    mlflow.log_metrics({
        'rmse': 0.85,
        'mae': 0.65,
        'hit_rate@10': 0.72,
        'ndcg@10': 0.68
    })
    
    # 4. ä¿å­˜æ¨¡å‹
    mlflow.sklearn.log_model(model, "model")
    
    # 5. è®°å½•æ•°æ®ç‰ˆæœ¬
    mlflow.log_param('data_date', '2025-03-15')
    mlflow.log_param('training_samples', len(train_data))
```

**æ¨¡å‹æ³¨å†Œå’Œéƒ¨ç½²**ï¼š
```python
# æ³¨å†Œæœ€ä½³æ¨¡å‹
model_uri = f"runs:/{run_id}/model"
mlflow.register_model(model_uri, "HybridRecommender")

# æ ‡è®°ä¸ºç”Ÿäº§ç‰ˆæœ¬
client = mlflow.tracking.MlflowClient()
client.transition_model_version_stage(
    name="HybridRecommender",
    version=3,
    stage="Production"
)

# APIä¸­åŠ è½½ç”Ÿäº§æ¨¡å‹
model = mlflow.sklearn.load_model("models:/HybridRecommender/Production")
```

**A/Bæµ‹è¯•**ï¼š
```python
# åŒæ—¶è¿è¡Œä¸¤ä¸ªæ¨¡å‹ç‰ˆæœ¬
model_a = load_model("models:/HybridRecommender/Production")
model_b = load_model("models:/HybridRecommender/Staging")

# éšæœºåˆ†é…ç”¨æˆ·
if user_id % 2 == 0:
    recommendations = model_a.recommend(user_id)
    mlflow.log_metric(f"user_{user_id}_model", 'A')
else:
    recommendations = model_b.recommend(user_id)
    mlflow.log_metric(f"user_{user_id}_model", 'B')
```

**ç›‘æ§ç”Ÿäº§æ¨¡å‹**ï¼š
- è·Ÿè¸ªAPIå“åº”æ—¶é—´
- è®°å½•æ¨èç‚¹å‡»ç‡
- ç›‘æ§æ¨¡å‹æ€§èƒ½é€€åŒ–"

---

## æŠ€æœ¯æ·±åº¦é—®é¢˜

### Q9: "What's the difference between your data lake (S3) and a data warehouse?"

| ç‰¹æ€§ | æ•°æ®æ¹– (S3) | æ•°æ®ä»“åº“ (Redshift) |
|------|------------|-------------------|
| Schema | Schema-on-Read | Schema-on-Write |
| æ•°æ®ç±»å‹ | åŸå§‹ã€éç»“æ„åŒ– | ç»“æ„åŒ–ã€èšåˆ |
| å¤„ç†æ–¹å¼ | ELT | ETL |
| æˆæœ¬ | ä½ | é«˜ |
| æŸ¥è¯¢æ€§èƒ½ | ä¸€èˆ¬ï¼ˆæ‰«æï¼‰ | ä¼˜ç§€ï¼ˆç´¢å¼•ï¼‰ |
| ç”¨é€” | å­˜å‚¨+æ¢ç´¢ | åˆ†ææŠ¥è¡¨ |

**åœ¨æˆ‘çš„é¡¹ç›®ä¸­**ï¼š
- S3å­˜å‚¨åŸå§‹è¯„åˆ†äº‹ä»¶ï¼ˆçµæ´»ã€ä½æˆæœ¬ï¼‰
- DynamoDBä½œä¸ºç‰¹å¾ä»“åº“ï¼ˆå¿«é€ŸæŸ¥è¯¢ï¼‰
- æœªæ¥å¯ä»¥æ·»åŠ Redshiftç”¨äºåˆ†ææŠ¥è¡¨

---

### Q10: "How do you ensure data quality?"

**å®Œç¾å›ç­”**:

"æˆ‘å®ç°äº†å¤šå±‚æ•°æ®è´¨é‡ä¿è¯ï¼š

**1. è¾“å…¥éªŒè¯**ï¼š
```python
def validate_event(event):
    # å¿…éœ€å­—æ®µæ£€æŸ¥
    required = ['user_id', 'movie_id', 'rating', 'timestamp']
    if not all(k in event for k in required):
        raise ValueError("Missing required fields")
    
    # æ•°æ®ç±»å‹æ£€æŸ¥
    assert isinstance(event['user_id'], int)
    assert isinstance(event['rating'], (int, float))
    
    # å€¼èŒƒå›´æ£€æŸ¥
    assert 1.0 <= event['rating'] <= 5.0
    
    return True
```

**2. æ•°æ®æ¸…æ´—**ï¼š
- å»é‡ï¼šåŸºäº(user_id, movie_id, timestamp)
- å¼‚å¸¸å€¼å¤„ç†ï¼šè¿‡æ»¤æç«¯è¯„åˆ†
- ç¼ºå¤±å€¼å¤„ç†ï¼šå¡«å……æˆ–åˆ é™¤

**3. SchemaéªŒè¯**ï¼ˆParquetï¼‰ï¼š
```python
schema = pa.schema([
    ('user_id', pa.int64()),
    ('avg_rating', pa.float64()),
    ('rating_count', pa.int64())
])

df.to_parquet('features.parquet', schema=schema)
```

**4. ç›‘æ§æŒ‡æ ‡**ï¼š
- æ¯æ—¥äº‹ä»¶æ•°é‡
- å¼‚å¸¸ç‡
- æ•°æ®å»¶è¿Ÿ
- ç‰¹å¾åˆ†å¸ƒ"

---

## å¿«é€Ÿå‚è€ƒ

### æ•°æ®æµæ€»ç»“

```
å®æ—¶æµ: Kafka â†’ S3 Bronze (JSON)
æ‰¹å¤„ç†: S3 Bronze â†’ Airflow â†’ S3 Silver (Parquet) + DynamoDB
æ¨è: DynamoDB â†’ FastAPI â†’ ç”¨æˆ·
```

### å…³é”®æ•°å­—

- **è¯„åˆ†çŸ©é˜µ**: 100 users Ã— 500 movies
- **ç›¸ä¼¼åº¦é‚»å±…**: K=20
- **æ¨èæ•°é‡**: Top-20
- **ç‰¹å¾æ›´æ–°**: æ¯å¤©æ‰¹å¤„ç†
- **APIå»¶è¿Ÿ**: <100ms (DynamoDBæŸ¥è¯¢)
- **æˆæœ¬**: ~$10/æœˆ (10ä¸‡ç”¨æˆ·è§„æ¨¡)

### æŠ€æœ¯å…³é”®è¯

é¢è¯•ä¸­è¦çªå‡ºçš„å…³é”®è¯ï¼š
- Lambda Architecture
- Data Lake (Bronze-Silver-Gold)
- Feature Store
- Hybrid Recommendation
- Collaborative Filtering
- Content-based Filtering
- Real-time Stream Processing
- Batch Processing
- Schema-on-Read
- MLflow Model Tracking

### é¡¹ç›®äº®ç‚¹

1. âœ… **å¯æ‰©å±•æ¶æ„**: SQLite â†’ AWSæ— ç¼åˆ‡æ¢
2. âœ… **Lambdaæ¶æ„**: å®æ—¶+æ‰¹å¤„ç†
3. âœ… **æ··åˆæ¨è**: ä¸‰ç§ç®—æ³•èåˆ
4. âœ… **å®Œæ•´æ•°æ®æµ**: Kafka â†’ S3 â†’ DynamoDB
5. âœ… **ç”Ÿäº§çº§ä»£ç **: é”™è¯¯å¤„ç†ã€æ—¥å¿—ã€ç›‘æ§
6. âœ… **é¢è¯•å‹å¥½**: æ¸…æ™°çš„æ¶æ„æ–‡æ¡£

ç¥é¢è¯•é¡ºåˆ©ï¼ğŸ¬ğŸš€


